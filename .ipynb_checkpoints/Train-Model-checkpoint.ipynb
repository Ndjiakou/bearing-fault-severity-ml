{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db07b33-0745-428c-9a7c-4d702c324c57",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Edge-Oriented Bearing Fault Severity Classification\n",
    "\n",
    "This script implements a window-based vibration signal processing\n",
    "pipeline using group-aware machine learning to estimate bearing\n",
    "fault severity from raw industrial bearing vibration data.\n",
    "\n",
    "Key components:\n",
    "- Window segmentation (fixed-length windows)\n",
    "- Time-domain feature extraction\n",
    "- Multi-class severity labeling\n",
    "- Group-aware train/test splitting (prevents leakage)\n",
    "- Group-based cross-validation\n",
    "- Random Forest classifier with feature importance analysis\n",
    "\n",
    "Designed for ML + Embedded Systems portfolio positioning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f5b41f-c80a-44d2-b8b2-ee0772e48f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Callable, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f979fc5-b669-404d-b9a1-5a8b0c6525b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# CONFIGURATION\n",
    "# ===============================\n",
    "\n",
    "NORMAL_DIR = \"Dataset/Normal\"\n",
    "FAULTY_DIR = \"Dataset/Faulty\"\n",
    "\n",
    "WINDOW_SIZE = 2048\n",
    "RANDOM_STATE = 42\n",
    "N_ESTIMATORS = 300\n",
    "MAX_DEPTH = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d641394a-2727-47d8-bd56-6d5c66096404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# LOGGING\n",
    "# ===============================\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0414e835-6810-4147-8926-e1148e2b1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# SEVERITY MAP\n",
    "# ===============================\n",
    "\n",
    "SEVERITY_MAP = {\n",
    "    \"normal\": 0,\n",
    "    \"ir007\": 1,   # Mild\n",
    "    \"ir014\": 2,   # Moderate\n",
    "    \"ir021\": 2,   # Moderate\n",
    "    \"ir028\": 3,   # Severe\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6412efc2-c641-4a61-991a-769b4b8a6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FEATURE EXTRACTION\n",
    "# ===============================\n",
    "\n",
    "def extract_features(signal: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Extract time-domain features from a vibration window.\"\"\"\n",
    "    return {\n",
    "        \"mean\": float(np.mean(signal)),\n",
    "        \"std\": float(np.std(signal)),\n",
    "        \"rms\": float(np.sqrt(np.mean(signal ** 2))),\n",
    "        \"peak_to_peak\": float(np.max(signal) - np.min(signal)),\n",
    "        \"energy\": float(np.sum(signal ** 2)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5aaa329-43ae-4b47-9270-bc1a186572dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# LOAD DRIVE END SIGNAL\n",
    "# ===============================\n",
    "\n",
    "def load_drive_end_signal(mat_file_path: str) -> np.ndarray:\n",
    "    \"\"\"Load Drive-End accelerometer signal from a MATLAB file.\"\"\"\n",
    "    data = loadmat(mat_file_path)\n",
    "    de_key = [key for key in data.keys() if key.endswith(\"_DE_time\")][0]\n",
    "    return data[de_key].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be12f09-199b-4d63-b2cd-a0288ae7000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# WINDOWING\n",
    "# ===============================\n",
    "\n",
    "def split_into_windows(signal: np.ndarray, window_size: int) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split signal into fixed-length non-overlapping windows.\n",
    "    Only full windows are kept to maintain consistent feature dimensions.\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    for start in range(0, len(signal) - window_size, window_size):\n",
    "        windows.append(signal[start:start + window_size])\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6419a14c-adf1-46fe-9401-91c656950ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# SEVERITY LABELING\n",
    "# ===============================\n",
    "\n",
    "def get_severity_label(filename: str) -> int:\n",
    "    \"\"\"Map filename to severity label based on fault diameter.\"\"\"\n",
    "    name = filename.lower()\n",
    "    for key, label in SEVERITY_MAP.items():\n",
    "        if key in name:\n",
    "            return label\n",
    "    logger.warning(f\"Skipping file with unknown severity: {filename}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a16c5b7a-e636-4d50-ade1-3934e99f6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# DIRECTORY PROCESSING\n",
    "# ===============================\n",
    "\n",
    "def process_directory(\n",
    "    directory: str,\n",
    "    label_function: Callable[[str], int]\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Process all .mat files in a directory and return feature rows.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".mat\"):\n",
    "            continue\n",
    "\n",
    "        label = label_function(filename)\n",
    "        if label is None:\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        signal = load_drive_end_signal(file_path)\n",
    "        windows = split_into_windows(signal, WINDOW_SIZE)\n",
    "\n",
    "        for w in windows:\n",
    "            features = extract_features(w)\n",
    "            features[\"label\"] = label\n",
    "            features[\"group\"] = os.path.splitext(filename)[0]\n",
    "            rows.append(features)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def build_dataset(normal_dir: str, faulty_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Build complete dataset from normal and faulty directories.\"\"\"\n",
    "    dataset = []\n",
    "    dataset += process_directory(normal_dir, lambda _: 0)\n",
    "    dataset += process_directory(faulty_dir, get_severity_label)\n",
    "    return pd.DataFrame(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c3a40c9-b926-4e26-86e9-997a12ce164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Building dataset...\n",
      "INFO:__main__:Class distribution:\n",
      "label\n",
      "0    828\n",
      "2    472\n",
      "1    237\n",
      "3    235\n",
      "Name: count, dtype: int64\n",
      "INFO:__main__:Performing group-based cross-validation...\n",
      "INFO:__main__:CV F1 scores: [1.         1.         0.99578052]\n",
      "INFO:__main__:Mean CV F1: 0.9986\n",
      "INFO:__main__:Training final model on training split...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Set Evaluation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       236\n",
      "           1       1.00      1.00      1.00       118\n",
      "           2       1.00      1.00      1.00        59\n",
      "           3       1.00      1.00      1.00        59\n",
      "\n",
      "    accuracy                           1.00       472\n",
      "   macro avg       1.00      1.00      1.00       472\n",
      "weighted avg       1.00      1.00      1.00       472\n",
      "\n",
      "Confusion Matrix:\n",
      "[[236   0   0   0]\n",
      " [  0 118   0   0]\n",
      " [  0   0  59   0]\n",
      " [  0   0   0  59]]\n",
      "\n",
      "Feature Importance:\n",
      "energy          0.265434\n",
      "rms             0.237707\n",
      "std             0.233124\n",
      "peak_to_peak    0.184449\n",
      "mean            0.079286\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# MAIN EXECUTION\n",
    "# ===============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger.info(\"Building dataset...\")\n",
    "    df = build_dataset(NORMAL_DIR, FAULTY_DIR)\n",
    "\n",
    "    logger.info(\"Class distribution:\\n%s\", df[\"label\"].value_counts())\n",
    "\n",
    "    X = df.drop(columns=[\"label\", \"group\"])\n",
    "    y = df[\"label\"]\n",
    "    groups = df[\"group\"]\n",
    "\n",
    "    # ===============================\n",
    "    # GROUP-AWARE SPLIT\n",
    "    # ===============================\n",
    "\n",
    "    gss = GroupShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=0.25,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    train_idx, test_idx = next(gss.split(X, y, groups))\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # ===============================\n",
    "    # ESTIMATOR (UNFITTED)\n",
    "    # ===============================\n",
    "\n",
    "    estimator = RandomForestClassifier(\n",
    "        n_estimators=N_ESTIMATORS,\n",
    "        max_depth=MAX_DEPTH,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # ===============================\n",
    "    # GROUP-BASED CROSS-VALIDATION\n",
    "    # ===============================\n",
    "\n",
    "    logger.info(\"Performing group-based cross-validation...\")\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "\n",
    "    cv_scores = cross_val_score(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=gkf,\n",
    "        groups=groups,\n",
    "        scoring=\"f1_macro\",\n",
    "    )\n",
    "\n",
    "    logger.info(\"CV F1 scores: %s\", cv_scores)\n",
    "    logger.info(\"Mean CV F1: %.4f\", np.mean(cv_scores))\n",
    "\n",
    "    # ===============================\n",
    "    # FINAL MODEL TRAINING\n",
    "    # ===============================\n",
    "\n",
    "    logger.info(\"Training final model on training split...\")\n",
    "    model = estimator.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"\\n=== Test Set Evaluation ===\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # ===============================\n",
    "    # FEATURE IMPORTANCE\n",
    "    # ===============================\n",
    "\n",
    "    importances = pd.Series(\n",
    "        model.feature_importances_,\n",
    "        index=X.columns\n",
    "    ).sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a7841-92b0-492b-9d7d-df88141521ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (lung38)",
   "language": "python",
   "name": "lung38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
